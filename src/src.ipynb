{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an API query to the Semantic Scholar API\n",
    "# to get the paper with the given ID\n",
    "# and print the title of the paper\n",
    "# along with its abstract and the titles of the\n",
    "# first 10 papers it cites\n",
    "\n",
    "\n",
    "def get_paper_info(paper_id, option):\n",
    "    #possible options include: id, title, abstract text, citations\n",
    "    \n",
    "    # Set the base URL for the Semantic Scholar API\n",
    "    base_url = \"https://api.semanticscholar.org/v1/paper/\"\n",
    "\n",
    "    # Set the full URL for the API query\n",
    "    url = base_url + paper_id\n",
    "\n",
    "    # Query the API\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Convert the response to JSON\n",
    "    data = json.loads(response.text)\n",
    "\n",
    "    # Print the title of the paper\n",
    "    title = str(data[\"title\"])\n",
    "\n",
    "    # Print the abstract of the paper\n",
    "    abstract = str(data[\"abstract\"])\n",
    "\n",
    "    # Put the first 10 paper IDs in an array\n",
    "    # and if there's an error, append `None\n",
    "    citations = []\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            citations.append(str(data[\"citations\"][i][\"paperId\"]))\n",
    "        except IndexError:\n",
    "            citations.append(None)\n",
    "\n",
    "    # return the options\n",
    "    if option == \"id\":\n",
    "        return paper_id\n",
    "    elif option == \"title\":\n",
    "        return title\n",
    "    elif option == \"abstract\":\n",
    "        return abstract\n",
    "    elif option == \"citations\":\n",
    "        return citations\n",
    "    else:\n",
    "        return \"Invalid option\"\n",
    "\n",
    "# Testing Function\n",
    "cit = get_paper_info(\"d4b651d6a904f69f8fa1dcad4ebe972296af3a9a\", \"citations\")[0]\n",
    "\n",
    "get_paper_info(\"d4b651d6a904f69f8fa1dcad4ebe972296af3a9a\", \"title\")\n",
    "get_paper_info(str(cit), \"title\")\n",
    "get_paper_info(str(cit), \"citations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_challenge_words(abstractString: str):\n",
    "    # Given an abstract,\n",
    "    # Returns array of strings of challenging words in the form:\n",
    "    # ['Active Galactic Nuclei', 'BPT classification', 'Quenching scenarios', ...]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardest words:  ['actively', 'typically', 'diagnostics', 'hosted', 'luminous', 'revisit', 'incompleteness', 'interestingly', 'consistently', 'unprecedented', 'redshifts', 'revisiting', 'diagnostics']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def hardest_words_synonym(text):\n",
    "    # This function extracts the hardest words\n",
    "    # Using the number of synonyms as a proxy\n",
    "    # for the level of difficulty of the word\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Identify the hardest words\n",
    "    hardest_words = []\n",
    "    for word in filtered_words:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if len(synonyms) == 1:\n",
    "            hardest_words.append(word)\n",
    "\n",
    "    return hardest_words\n",
    "\n",
    "# Input text\n",
    "text = \"Nebular He iiùúÜ4686√Ö line emission is useful to unveil active galactic nuclei (AGN) residing in actively star-forming (SF) galaxies, typically missed by the standard BPT classification. Here we adopt the He ii diagnostic to identify hidden AGN in the Local Universe using for the first time spatially-resolved data from the Data Release 15 of the Mapping Nearby Galaxies at APO survey (MaNGA DR15). By combining results from He ii and BPT diagnostics, we overall select 459 AGN host candidates (‚àº10% in MaNGA DR15), out of which 27 are identified as AGN by the He ii diagram only. The He ii-only AGN population is hosted by massive (M‚àó & 1010 M\f",
    ") SF Main Sequence galaxies, and on average less luminous than the BPT-selected AGN. Given the He ii line faintness, we revisit our census accounting for incompleteness effects due to the He ii sensitivity limit of MaNGA. We thus obtain an overall increased fraction (11%) of AGN in MaNGA compared to the BPT-only census (9%), which further increases to 14% for galaxies more massive than 1010 M\f",
    "; interestingly, on the SF Main Sequence the increase is by about a factor of 2. A substantial number of AGN in SF galaxies points to significant, coeval star formation and black hole accretion, consistently with results from hydrodynamical simulations and with important implications on quenching scenarios. In view of exploring unprecedented high redshifts with JWST and new ground-based facilities, revisiting the standard BPT classification through novel emission-line diagnostics is fundamental to discover AGN in highly SF environments.\"\n",
    "\n",
    "# Find the hardest words\n",
    "hardest = hardest_words(text)\n",
    "\n",
    "# Print the hardest words\n",
    "print(\"Hardest words: \", hardest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dj/yd_369kx231256n8rxsw9wlh0000gn/T/ipykernel_79114/784261223.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
